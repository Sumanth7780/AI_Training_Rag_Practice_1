{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing DPR using Transformer's(Hugging face) library. \n",
    "1. We will load Pre-Trained Models\n",
    "2. We will encode Passage and query\n",
    "3. Retrieve the most relevant passage based on semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to read a PDF file and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Split the text into passages of given length\n",
    "def split_text_into_passages(text, passage_length=100):\n",
    "    words = text.split()\n",
    "    passages = []\n",
    "    \n",
    "    for i in range(0, len(words), passage_length):\n",
    "        passages.append(\" \".join(words[i:i+passage_length]))\n",
    "    return passages\n",
    "\n",
    "# List to store all passages\n",
    "p = []\n",
    "\n",
    "# Paths to the PDF files\n",
    "pdf_path_1 = r\"microsoft-annual-report.pdf\"\n",
    "pdf_path_2 = r\"Agthia-Annual-Report-2023-EN.pdf\"\n",
    "pdf_path_3 = r\"ADNOC Distribution Q3 23 Financial Statements_Eng.pdf\"\n",
    "\n",
    "# Extract text from all three PDFs\n",
    "pdf_text_1 = extract_text_from_pdf(pdf_path_1)\n",
    "pdf_text_2 = extract_text_from_pdf(pdf_path_2)\n",
    "pdf_text_3 = extract_text_from_pdf(pdf_path_3)\n",
    "\n",
    "# Combine all extracted text\n",
    "combined_text = pdf_text_1 + \" \" + pdf_text_2 + \" \" + pdf_text_3\n",
    "\n",
    "# Split the combined text into passages\n",
    "passages = split_text_into_passages(combined_text, passage_length=100)\n",
    "\n",
    "# Store the first 100 passages in list p\n",
    "for i, passage in enumerate(passages[:100]):\n",
    "    p.append(passage)\n",
    "\n",
    "# p now contains the first 100 passages from the combined PDF texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "C:\\Users\\suman\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "\n",
    "#Load Query Encoder and Tokenizer\n",
    "query_encoder=DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "query_tokenizer=DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "passage_encoder =DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "passage_tokenizer=DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "passage_inputs = passage_tokenizer(p,return_tensors='pt',padding=True,truncation=True)\n",
    "passage_vectors= passage_encoder(**passage_inputs).pooler_output\n",
    "\n",
    "print(passage_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repurchase program commenced in November 2021, following completion of the program approved on September 18, 2019, has no expiration date, and may be terminated at any time. As of June 30, 2023, $22.3 billion remained of this $60.0 billion share repurchase program. We repurchased the following shares of common stock under the share repurchase programs: All repurchases were made using cash resources. Shares repurchased during fiscal year 2023 and the fourth and third quarters of fiscal year 2022 were under the share repurchase program approved on September 14, 2021. Shares repurchased during the second quarter of fiscal year 2022 were\n",
      "a range of products and services. Growth depends on our ability to reach new users, add value to our core product set, and continue to expand our product and service offerings into new markets. Office Consumer revenue is mainly affected by the percentage of customers that buy Office with their new devices and the continued shift from Office licensed on-premises to Microsoft 365 Consumer subscriptions. Office Consumer Services revenue is mainly affected by the demand for communication and storage through Skype, Outlook.com, and OneDrive, which is largely driven by subscriptions, advertising, and the sale of minutes. LinkedIn LinkedIn connects the\n"
     ]
    }
   ],
   "source": [
    "query= input(\"What's your query?\")\n",
    "query_input=query_tokenizer(query,return_tensors='pt')\n",
    "query_vector=query_encoder(**query_input).pooler_output\n",
    "\n",
    "#Similarty search with Query and passage\n",
    "import torch\n",
    "similarity_scores=torch.matmul(query_vector,passage_vectors.T)\n",
    "top_k=torch.topk(similarity_scores,k=2)\n",
    "\n",
    "#Display the top passages\n",
    "for idx in top_k.indices[0]:\n",
    "    print(passages[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
